{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedf3dad-2a14-4575-85d4-2a76ace5e5cd",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained T5 (25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Pretrained T5. Воспользуйтесь https://huggingface.co/docs/transformers/model_doc/t5 предобученной моделью. В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Не забудьте важные аспекты обучения модели:\n",
    "* Взять готовый t5 токенизатор\n",
    "* Resize matrix embedding - скорей всего ваша матрица эмбеддингов не будет включать эмбеддинги из вашего сета. Пример обновления матрицы эмбеддингов тут тут https://github.com/runnerup96/Transformers-Tuning/blob/main/t5_encoder_decoder.py\n",
    "* Learning rate schedualer/Adafactor with constant learning rate\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, pretrained model name\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d2f4ce-23dd-4bd5-b019-1cead0dd3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c473a1-57b8-411a-849a-4459664e062f",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a66973e-41c6-45e1-89ea-317e319a120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import sys, os\n",
    "import importlib\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"./src_t5\"))\n",
    "\n",
    "from data.datamodule import DataManager\n",
    "\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94419942-9826-447a-93af-bb27168fd95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \",\n",
    "    \"i m \",\n",
    "    \"he is\",\n",
    "    \"he s \",\n",
    "    \"she is\",\n",
    "    \"she s \",\n",
    "    \"you are\",\n",
    "    \"you re \",\n",
    "    \"we are\",\n",
    "    \"we re \",\n",
    "    \"they are\",\n",
    "    \"they re \",\n",
    ")\n",
    "\n",
    "def filter_func(x):\n",
    "    MAX_LENGTH = 15\n",
    "    len_filter = lambda x: len(x[0].split(\" \")) <= MAX_LENGTH and len(x[1].split(\" \")) <= MAX_LENGTH\n",
    "    eng_prefix_filter = lambda x: x[0].startswith(eng_prefixes)\n",
    "    rus_prefix_filter = lambda x: x[0].startswith(rus_prefixes)\n",
    "    return len_filter(x) and prefix_filter(x)\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 64,          # <--- size of batch\n",
    "    \"num_workers\": 47,          # <--- num cpu to use in dataloader\n",
    "    \"prefix_filter\": eng_prefixes,      # <--- callable obj to filter data\n",
    "    \"max_length\": 15,\n",
    "    \"filename\": \"./rus.txt\",    # <--- path to file with sentneces\n",
    "    \"lang1\": \"en\",              # <--- name of the first lang    \n",
    "    \"lang2\": \"ru\",              # <--- name of the second lang\n",
    "    \"reverse\": False,           # <--- direct or reverse order in pairs\n",
    "    \"train_size\": 0.8,          # <--- ratio of data pairs to use in train\n",
    "    \"run_name\": \"tutorial\",     # <--- run name to logger and checkpoints\n",
    "    \"quantile\": 0.95,           # <--- (1 - quantile) longest sentences will be removed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3eec29-7719-4d9b-995f-838556f77994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 496059/496059 [00:05<00:00, 83449.68it/s]\n",
      "/home/krotovan/hw-sber/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<bound method DataManager.train_dataloader of <data.datamodule.DataManager object at 0x7feca718a4d0>>,\n",
       " <bound method DataManager.val_dataloader of <data.datamodule.DataManager object at 0x7feca718a4d0>>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DataManager(config, device)\n",
    "dm.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89665491-4343-47b0-8a08-62770439024e",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74986793-aa8d-4609-88dc-faf9d68b98e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "spec not found for the module 'models.seq2seq_t5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seq2seq_t5\n\u001b[0;32m----> 2\u001b[0m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq2seq_t5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:168\u001b[0m, in \u001b[0;36mreload\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    166\u001b[0m spec \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m__spec__ \u001b[38;5;241m=\u001b[39m _bootstrap\u001b[38;5;241m.\u001b[39m_find_spec(name, pkgpath, target)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspec not found for the module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m    169\u001b[0m _bootstrap\u001b[38;5;241m.\u001b[39m_exec(spec, module)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The module may have replaced itself in sys.modules!\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: spec not found for the module 'models.seq2seq_t5'"
     ]
    }
   ],
   "source": [
    "from models import seq2seq_t5\n",
    "importlib.reload(seq2seq_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed8f38c-d141-427e-9fd8-8a6afeca108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq_t5.Seq2SeqT5(\n",
    "        model=\"google-t5/t5-small\",\n",
    "      max_len=15,\n",
    "           lr=1e-3,\n",
    "    tokenizer=dm.tokenizer,\n",
    "       device=device\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c5acf7-b64a-4153-bdef-f06cb8cd2cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# TB Logger\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=config[\"run_name\"])\n",
    "\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class CustomWriter(Callback):\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        print(\"Training is started!\")\n",
    "        \n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"Training is done.\")\n",
    "        \n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print('\\n\\nExample:')\n",
    "        pl_module.eval()\n",
    "        # phrase = 'but when you consider that a human being has the opportunity of being acquainted with'\n",
    "        phrase = 'translate English to Russian: between the lines, its clear that Tom isnt having such'\n",
    "        print(phrase)\n",
    "        in_tokens = pl_module.tokenizer(phrase)\n",
    "        prediction = pl_module.predict(torch.Tensor([in_tokens.input_ids]).to(pl_module.device).long(), torch.Tensor([in_tokens.attention_mask]).to(pl_module.device).long())\n",
    "        print(pl_module.tokenizer.decode(prediction[0], skip_special_tokens=True))\n",
    "        pl_module.train()\n",
    "        print()\n",
    "        \n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=\"runs/{}/\".format(config[\"run_name\"]),\n",
    "    filename=\"{epoch:02d}-{step:d}-{val_loss:.4f}\",\n",
    "    verbose=True,\n",
    "    every_n_epochs=1,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "# Initialize a Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=8,\n",
    "    min_epochs=1,\n",
    "    devices=[6],\n",
    "    callbacks=[lr_monitor, checkpoint_callback, CustomWriter()],\n",
    "    check_val_every_n_epoch=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ab94ed-b593-4b8a-b839-21e95eb861eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 496059/496059 [00:05<00:00, 85223.25it/s]\n",
      "/home/krotovan/hw-sber/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/krotovan/hw-sber/env/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/krotovan/hw-sber/pytorch-project/runs/tutorial exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name     | Type                       | Params\n",
      "--------------------------------------------------------\n",
      "0 | t5_model | T5ForConditionalGeneration | 60.5 M\n",
      "--------------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "241.969   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krotovan/hw-sber/env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/krotovan/hw-sber/env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/home/krotovan/hw-sber/pytorch-project/./src_t5/data/mt_dataset.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  source_ids, attention_masks, target_ids = torch.tensor(self.tokenized_source_list[idx]     ).to(self.device), \\\n",
      "/home/krotovan/hw-sber/pytorch-project/./src_t5/data/mt_dataset.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.attention_mask_source_list[idx]).to(self.device), \\\n",
      "/home/krotovan/hw-sber/pytorch-project/./src_t5/data/mt_dataset.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.tokenized_target_list[idx]     ).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krotovan/hw-sber/env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34481994d26f485cad9aebc1ef8aff47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 3765: 'val_loss' reached 0.50890 (best 0.50890), saving model to '/home/krotovan/hw-sber/pytorch-project/runs/tutorial/epoch=00-step=3765-val_loss=0.5089.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example:\n",
      "translate English to Russian: between the lines, its clear that Tom isnt having such\n",
      "меду л то у\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 7530: 'val_loss' reached 0.48172 (best 0.48172), saving model to '/home/krotovan/hw-sber/pytorch-project/runs/tutorial/epoch=01-step=7530-val_loss=0.4817.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example:\n",
      "translate English to Russian: between the lines, its clear that Tom isnt having such\n",
      "меду лди вно\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 11295: 'val_loss' reached 0.44756 (best 0.44756), saving model to '/home/krotovan/hw-sber/pytorch-project/runs/tutorial/epoch=02-step=11295-val_loss=0.4476.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example:\n",
      "translate English to Russian: between the lines, its clear that Tom isnt having such\n",
      "меду текст вно\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 15060: 'val_loss' reached 0.41634 (best 0.41634), saving model to '/home/krotovan/hw-sber/pytorch-project/runs/tutorial/epoch=03-step=15060-val_loss=0.4163.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example:\n",
      "translate English to Russian: between the lines, its clear that Tom isnt having such\n",
      "меду лестни сно\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 18825: 'val_loss' reached 0.40786 (best 0.40786), saving model to '/home/krotovan/hw-sber/pytorch-project/runs/tutorial/epoch=04-step=18825-val_loss=0.4079.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example:\n",
      "translate English to Russian: between the lines, its clear that Tom isnt having such\n",
      "меду линии вно\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 22590: 'val_loss' reached 0.39761 (best 0.39761), saving model to '/home/krotovan/hw-sber/pytorch-project/runs/tutorial/epoch=05-step=22590-val_loss=0.3976.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example:\n",
      "translate English to Russian: between the lines, its clear that Tom isnt having such\n",
      "меду текста сно\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example:\n",
      "translate English to Russian: between the lines, its clear that Tom isnt having such\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 26355: 'val_loss' reached 0.39466 (best 0.39466), saving model to '/home/krotovan/hw-sber/pytorch-project/runs/tutorial/epoch=06-step=26355-val_loss=0.3947.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "меду текст сно\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 30120: 'val_loss' reached 0.38923 (best 0.38923), saving model to '/home/krotovan/hw-sber/pytorch-project/runs/tutorial/epoch=07-step=30120-val_loss=0.3892.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example:\n",
      "translate English to Russian: between the lines, its clear that Tom isnt having such\n",
      "меду линии сно\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ffef5-8732-4441-b187-fcc0f05c8005",
   "metadata": {},
   "source": [
    "### Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364a69d5-4b5c-4c03-9c62-6bcc222de245",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"./eng2ru-t5-translator-0.2bleu.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d338cc-b6a8-46da-b853-acd133105e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./eng2ru-t5-translator-0.2bleu.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
