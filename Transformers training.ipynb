{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание Transformers Training (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании требуется обучить несколько Transformer-based моделей в задаче машинного перевода. Для обучения можно воспользоваться текущим проектом, так и реализовать свой пайплайн обучения. Если будете использовать проект, теги **TODO** проекта отмечают, какие компоненты надо реализовать.\n",
    "В ноутбуке нужно только отобразить результаты обучения и выводы. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Ваш код обучения нужно выложить на ваш github, в строке ниже дать ссылку на него. В первую очередь будут оцениваться результаты в ноутбуке, код нужен для проверки адекватности результатов. \n",
    "\n",
    "Обучать модели до конца не нужно, только для демонстрации, что модель обучается и рабочая - снижение val_loss, рост bleu_score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сcылка на ваш github с проектом(вставить свой) - https://github.com/runnerup96/pytorch-machine-translation\n",
    "\n",
    "Ноутбук с результатами выкладывать на ваш **google диск** курса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "`\n",
    "wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip\n",
    "`\n",
    "\n",
    "Модели нужно обучить на задаче перевода с английского на русский. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение Seq2seq Transformer модель(25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Transformer. В качестве блока трансформера можно использовать https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html. В качестве токенизатора воспользуйтесь HuggingFace токенизатор для source/target языков - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. \n",
    "\n",
    "Не забудьте остальные элементы модели:\n",
    "* Мы можем использовать 1 трансформер как энкодер - декодером будет выступать линейный слой. \n",
    "* Обучите свой BPE токенизатор - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "* Матрицу эмбеддингов токенов\n",
    "* Матрицу позицонных эмбеддингов\n",
    "* Линейный слой проекции в target словарь\n",
    "* Функцию маскирования будущих состояний attention, так как модель авто-регрессионна\n",
    "* Learning rate schedualer\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, размерность скрытого слоя, количетсво слоев\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представлено две реализации, с разным подходом к обучению\n",
    " - `seq2seq_transformer` через forward на src + tgt\n",
    " - `seq2seq_transformer2` через forward на src, в predict формате"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last checkpoint path:```./Transformer-model-project/eng2ru-transformer-translator-0.054bleu.ckpt```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import importlib\n",
    "import torch\n",
    "sys.path.append(os.path.join(os.getcwd(), \"./Transformer-model-project/src\"))\n",
    "\n",
    "from data.datamodule import DataManager\n",
    "\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \",\n",
    "    \"i m \",\n",
    "    \"he is\",\n",
    "    \"he s \",\n",
    "    \"she is\",\n",
    "    \"she s \",\n",
    "    \"you are\",\n",
    "    \"you re \",\n",
    "    \"we are\",\n",
    "    \"we re \",\n",
    "    \"they are\",\n",
    "    \"they re \",\n",
    ")\n",
    "\n",
    "def filter_func(x):\n",
    "    MAX_LENGTH = 15\n",
    "    len_filter = lambda x: len(x[0].split(\" \")) <= MAX_LENGTH and len(x[1].split(\" \")) <= MAX_LENGTH\n",
    "    eng_prefix_filter = lambda x: x[0].startswith(eng_prefixes)\n",
    "    rus_prefix_filter = lambda x: x[0].startswith(rus_prefixes)\n",
    "    return len_filter(x) and prefix_filter(x)\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 64,          # <--- size of batch\n",
    "    \"num_workers\": 47,          # <--- num cpu to use in dataloader\n",
    "    \"prefix_filter\": eng_prefixes,      # <--- callable obj to filter data\n",
    "    \"max_length\": 15,\n",
    "    \"filename\": \"./rus.txt\",    # <--- path to file with sentneces\n",
    "    \"lang1\": \"en\",              # <--- name of the first lang    \n",
    "    \"lang2\": \"ru\",              # <--- name of the second lang\n",
    "    \"reverse\": False,           # <--- direct or reverse order in pairs\n",
    "    \"train_size\": 0.8,          # <--- ratio of data pairs to use in train\n",
    "    \"run_name\": \"tutorial\",     # <--- run name to logger and checkpoints\n",
    "    \"quantile\": 0.95,           # <--- (1 - quantile) longest sentences will be removed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|██████████| 496059/496059 [00:03<00:00, 164368.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Space tokenizer fitted - 18831 tokens\n",
      "\n",
      "\n",
      "\n",
      "Space tokenizer fitted - 30000 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<bound method DataManager.train_dataloader of <data.datamodule.DataManager object at 0x14c68b620>>,\n",
       " <bound method DataManager.val_dataloader of <data.datamodule.DataManager object at 0x14c68b620>>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DataManager(config, device)\n",
    "dm.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.seq2seq_transformer2' from '/Users/a.i.krotov/Desktop/universal_osa/contest/hw-2/./Transformer-model-project/src/models/seq2seq_transformer2.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import seq2seq_transformer2\n",
    "importlib.reload(seq2seq_transformer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Transformer-model-project/eng2ru-transformer-translator-0.054bleu.ckpt\"\n",
    "model = seq2seq_transformer2.Seq2SeqTransformer.load_from_checkpoint(\n",
    "       checkpoint,\n",
    "               lr = 1e-3,\n",
    "            nhead = 4,\n",
    "          src_dim = dm.input_lang_n_words,  # SV\n",
    "          tgt_dim = dm.output_lang_n_words, # TV\n",
    "          emb_dim = 256,\n",
    "          hdn_dim = 256, # dim_feedforward\n",
    "      enc_nlayers = 3,\n",
    "      dec_nlayers = 3,\n",
    "    tgt_tokenizer = dm.target_tokenizer,\n",
    "    src_tokenizer = dm.source_tokenizer,\n",
    "          dropout = 0.2,\n",
    "          max_len = 15,\n",
    "      tgt_pad_idx = dm.target_tokenizer.word2index['PAD'],\n",
    "      tgt_sos_idx = dm.target_tokenizer.word2index['SOS'],\n",
    "      tgt_eos_idx = dm.target_tokenizer.word2index['EOS'],\n",
    "      src_pad_idx = dm.source_tokenizer.word2index['PAD'],\n",
    "      src_sos_idx = dm.source_tokenizer.word2index['SOS'],\n",
    "      src_eos_idx = dm.source_tokenizer.word2index['EOS']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры обучения модели\n",
    "```python\n",
    "      # learning rate = 1e-3\n",
    "         # batch size = 128\n",
    "          # epoch num = 30\n",
    "         # hidden dim = 256\n",
    "# encoding num layers = 3\n",
    "# decoding num layers = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логи метрик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы взаимодействуете с Моделью №71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=./Transformer-model-project/lightning_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры перевода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56848, 258341, 297378, 43599, 173832, 122408, 610, 240124, 86545, 199174]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "idx_list = [random.randint(0, len(dm.source_sentences)-1) for _ in range(10)]\n",
    "idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng:  does that satisfy you\n",
      "rus target:  тебя это устраивает\n",
      "rus predicted:  это удовлетво удовлетво удовлетво удовлетво удовлетво удовлетво\n",
      "\n",
      "eng:  you only have to follow the instructions\n",
      "rus target:  вам надо просто следовать инструкциям\n",
      "rus predicted:  тебе должен следовать инструкциям инструкциям инструкциям инструкциям\n",
      "\n",
      "eng:  i suppose you know why tom didn t do what we asked him to do\n",
      "rus target:  я полагаю вы знаете почему том не сделал то что мы просили его сделать\n",
      "rus predicted:  полагаю почему почему почему почему почему почему почему почему почему почему почему почему почему\n",
      "\n",
      "eng:  tom says he ll come\n",
      "rus target:  том говорит что придёт\n",
      "rus predicted:  том говорит что придёт\n",
      "\n",
      "eng:  tom is naked from the waist up\n",
      "rus target:  том голый по пояс\n",
      "rus predicted:  том голый голый голый пояс\n",
      "\n",
      "eng:  this is a question for tom\n",
      "rus target:  это вопрос к тому\n",
      "rus predicted:  это вопрос вопрос вопрос вопрос вопрос\n",
      "\n",
      "eng:  sit still\n",
      "rus target:  сиди спокойно\n",
      "rus predicted:  садись ещё\n",
      "\n",
      "eng:  we ll never find tom in this blizzard\n",
      "rus target:  в такую метель нам тома не найти\n",
      "rus predicted:  мы никогда не узнаем найдём тома тома\n",
      "\n",
      "eng:  tom isn t always around\n",
      "rus target:  том не всегда поблизости\n",
      "rus predicted:  том не всегда поблизости\n",
      "\n",
      "eng:  as was expected he won the prize\n",
      "rus target:  как и предполагалось он получил приз\n",
      "rus predicted:  он был приз он получил приз\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate(phrase):\n",
    "    model.eval()\n",
    "    prediction = model.predict(phrase)\n",
    "    model.train()\n",
    "    return prediction\n",
    "\n",
    "for idx in idx_list:\n",
    "    print('eng: ', dm.source_sentences[idx])\n",
    "    print('rus target: ', dm.target_sentences[idx])\n",
    "    print('rus predicted: ', translate(dm.source_sentences[idx]))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained T5 (25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Pretrained T5. Воспользуйтесь https://huggingface.co/docs/transformers/model_doc/t5 предобученной моделью. В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Не забудьте важные аспекты обучения модели:\n",
    "* Взять готовый t5 токенизатор\n",
    "* Resize matrix embedding - скорей всего ваша матрица эмбеддингов не будет включать эмбеддинги из вашего сета. Пример обновления матрицы эмбеддингов тут тут https://github.com/runnerup96/Transformers-Tuning/blob/main/t5_encoder_decoder.py\n",
    "* Learning rate schedualer/Adafactor with constant learning rate\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, pretrained model name\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last checkpoint path:```./T5-model-project/eng2ru-t5-translator-0.2bleu.ckpt```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import importlib\n",
    "import torch\n",
    "sys.path.append(os.path.join(os.getcwd(), \"./T5-model-project/src_t5\"))\n",
    "\n",
    "from data.datamodule import DataManager\n",
    "\n",
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \",\n",
    "    \"i m \",\n",
    "    \"he is\",\n",
    "    \"he s \",\n",
    "    \"she is\",\n",
    "    \"she s \",\n",
    "    \"you are\",\n",
    "    \"you re \",\n",
    "    \"we are\",\n",
    "    \"we re \",\n",
    "    \"they are\",\n",
    "    \"they re \",\n",
    ")\n",
    "\n",
    "def filter_func(x):\n",
    "    MAX_LENGTH = 15\n",
    "    len_filter = lambda x: len(x[0].split(\" \")) <= MAX_LENGTH and len(x[1].split(\" \")) <= MAX_LENGTH\n",
    "    eng_prefix_filter = lambda x: x[0].startswith(eng_prefixes)\n",
    "    rus_prefix_filter = lambda x: x[0].startswith(rus_prefixes)\n",
    "    return len_filter(x) and prefix_filter(x)\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 64,          # <--- size of batch\n",
    "    \"num_workers\": 47,          # <--- num cpu to use in dataloader\n",
    "    \"prefix_filter\": eng_prefixes,      # <--- callable obj to filter data\n",
    "    \"max_length\": 15,\n",
    "    \"filename\": \"./rus.txt\",    # <--- path to file with sentneces\n",
    "    \"lang1\": \"en\",              # <--- name of the first lang    \n",
    "    \"lang2\": \"ru\",              # <--- name of the second lang\n",
    "    \"reverse\": False,           # <--- direct or reverse order in pairs\n",
    "    \"train_size\": 0.8,          # <--- ratio of data pairs to use in train\n",
    "    \"run_name\": \"tutorial\",     # <--- run name to logger and checkpoints\n",
    "    \"quantile\": 0.95,           # <--- (1 - quantile) longest sentences will be removed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 496059/496059 [00:05<00:00, 83608.40it/s]\n",
      "/home/krotovan/hw-sber/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<bound method DataManager.train_dataloader of <data.datamodule.DataManager object at 0x7f5d4fda5000>>,\n",
       " <bound method DataManager.val_dataloader of <data.datamodule.DataManager object at 0x7f5d4fda5000>>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = DataManager(config, device)\n",
    "dm.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.seq2seq_t5' from '/home/krotovan/hw-sber/pytorch-project/./T5-model-project/src_t5/models/seq2seq_t5.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models import seq2seq_t5\n",
    "importlib.reload(seq2seq_t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./T5-model-project/eng2ru-t5-translator-0.2bleu.ckpt\"\n",
    "model = seq2seq_t5.Seq2SeqT5.load_from_checkpoint(checkpoint, tokenizer=dm.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры обучения модели\n",
    "```python\n",
    "        # learning rate = 1e-3\n",
    "           # batch size = 64\n",
    "            # epoch num = 8\n",
    "# pretrained model name = \"google-t5/t5-small\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логи метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=./T5-model-project/lightning_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры перевода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[251088, 210897, 224177, 117624, 124101, 240902, 99409, 194512, 121185, 271286]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "idx_list = [random.randint(0, len(dm.source_sentences)-1) for _ in range(10)]\n",
    "idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng:  tom and mary said they were on our side\n",
      "rus target:  том и мэри сказали что они на нашей стороне\n",
      "rus predicted:  том  с скаали\n",
      "\n",
      "eng:  i hope i can pass the driving test\n",
      "rus target:  надеюсь я смогу сдать экзамен по вождению\n",
      "rus predicted:  надес  смоу сда\n",
      "\n",
      "eng:  tom put the thermometer on the wall\n",
      "rus target:  том повесил термометр на стену\n",
      "rus predicted:  том том овесил т\n",
      "\n",
      "eng:  i hate standing on the bus\n",
      "rus target:  ненавижу стоять в автобусе\n",
      "rus predicted:  стот в автоусе\n",
      "\n",
      "eng:  tom laughed good naturedly\n",
      "rus target:  том добродушно рассмеялся\n",
      "rus predicted:  том том дораи\n",
      "\n",
      "eng:  call me immediately after you meet him\n",
      "rus target:  позвони мне сразу как только с ним встретишься\n",
      "rus predicted:  овони мне не сра\n",
      "\n",
      "eng:  tom says that he s tired\n",
      "rus target:  том говорит что устал\n",
      "rus predicted:  том оворит то\n",
      "\n",
      "eng:  the money on the desk isn t mine\n",
      "rus target:  деньги на столе не мои\n",
      "rus predicted:  дении на столеле не\n",
      "\n",
      "eng:  she reached out for my arm\n",
      "rus target:  она потянулась к моей руке\n",
      "rus predicted:  она оттнулас\n",
      "\n",
      "eng:  could you watch tom while i run to the store\n",
      "rus target:  посидишь с томом пока я схожу в магазин\n",
      "rus predicted:  в мааину осиди\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate(phrase):\n",
    "    model.eval()\n",
    "    in_tokens = model.tokenizer(phrase)\n",
    "    in_tokens_input_ids      = torch.Tensor([in_tokens.input_ids     ]).to(model.device).long()\n",
    "    in_tokens_attention_mask = torch.Tensor([in_tokens.attention_mask]).to(model.device).long()\n",
    "    prediction = model.predict(in_tokens_input_ids, in_tokens_attention_mask)\n",
    "    model.train()\n",
    "    return model.tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
    "    \n",
    "for idx in idx_list:\n",
    "    print('eng: ', dm.source_sentences[idx])\n",
    "    print('rus target: ', dm.target_sentences[idx])\n",
    "    print('rus predicted: ', translate(dm.target_sentences[idx]))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [мем к переводу](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/%D0%AD%D1%82%D0%BE_%D0%BD%D0%B5%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE%2C_%D0%BD%D0%BE_%D1%8D%D1%82%D0%BE_%D1%87%D0%B5%D1%81%D1%82%D0%BD%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.jpg/800px-%D0%AD%D1%82%D0%BE_%D0%BD%D0%B5%D0%BC%D0%BD%D0%BE%D0%B3%D0%BE%2C_%D0%BD%D0%BE_%D1%8D%D1%82%D0%BE_%D1%87%D0%B5%D1%81%D1%82%D0%BD%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.jpg?20220728114737)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
